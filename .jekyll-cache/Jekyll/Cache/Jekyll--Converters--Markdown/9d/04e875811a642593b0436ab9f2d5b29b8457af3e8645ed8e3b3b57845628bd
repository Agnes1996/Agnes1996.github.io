I"<p>##相关性分析
.cov协方差矩阵
.corr相关系数矩阵,Pearson、Spearman、kendall等系数
通用（general）的分类模型一直是机器学习研究的一个活跃的领域，并且有统计和传统机器学习两拨人用两类不同的方法论在搞（有人叫做统计学习和机器学习两个派）。</p>

<p>#机器学习算法分类</p>

<p>#分类一</p>
<h2 id="监督学习训练数据有标签">监督学习：训练数据有标签</h2>
<h2 id="非监督学训练数据没有任何标记">非监督学：训练数据没有任何标记</h2>
<ul>
  <li>聚类分析</li>
  <li>数据降维处理 ：特征提取、特征压缩，方便可视化</li>
  <li>异常检测
    <h2 id="半监督学习一部分数据有标记一部分没有">半监督学习：一部分数据有标记，一部分没有</h2>
    <p>通常先使用无监督学习手段对数据做处理，之后使用监督学习手段做模型的训练和预测</p>
    <h2 id="增强学习">增强学习</h2>
  </li>
</ul>

<p>#分类二</p>
<h2 id="在线学习新数据带来的输出结果会作为输入">在线学习：新数据带来的输出结果会作为输入</h2>
<p>优点：及时反映环境变化
问题：新数据可能带来不好的变化</p>

<h2 id="批量学习新数据不改变模型">批量学习：新数据不改变模型</h2>
<p>优点：简单
问题：难以适应环境变化</p>

<p>#分类三</p>

<p>##参数学习：一旦学到参数，就不在需要原有数据集</p>

<p>##非参数学习：不对模型进行过多假设</p>

<p>##</p>

<h2 id="knn">kNN</h2>

<p>优点 ：1）KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练2）KNN理论简单，容易实现</p>

<p>缺点：1）对于样本容量大的数据集计算量比较大。2）样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多。3）KNN每一次分类都会重新进行一次全局运算。4）k值大小的选择。应用领域：文本分类、模式识别、聚类分析，多分类领域</p>

<p>线性回归
多项式回归
逻辑回归
模型正则化
PCA 特征压缩
##SVM
支持向量机是一种基于分类边界的方法。其基本原理是（以二维数据为例）：如果训练数据分布在二维平面上的点，它们按照其分类聚集在不同的区域。基于分类边界的分类算法的目标是，通过训练，找到这些分类之间的边界（直线的――称为线性划分，曲线的――称为非线性划分）。对于多维数据（如N维），可以将它们视为N维空间中的点，而分类边界就是N维空间中的面，称为超面（超面比N维空间少一维）。线性分类器使用超平面类型的边界，非线性分类器使用超曲面。支持向量机的原理是将低维空间的点映射到高维空间，使它们成为线性可分，再使用线性划分的原理来判断分类边界。在高维空间中是一种线性划分，而在原有的数据空间中，是一种非线性划分。</p>

<p>优点：1）解决小样本下机器学习问题。2）解决非线性问题。3）无局部极小值问题。（相对于神经网络等算法）4）可以很好的处理高维数据集。5）泛化能力比较强。</p>

<p>缺点：1）对于核函数的高维映射解释力不强，尤其是径向基函数。2）对缺失数据敏感。应用领域：文本分类、图像识别、主要二分类领域。</p>

<p>##决策树
决策树优点：1、决策树易于理解和解释，可以可视化分析，容易提取出规则。2、可以同时处理标称型和数值型数据。3、测试数据集时，运行速度比较快。4、决策树可以很好的扩展到大型数据库中，同时它的大小独立于数据库大小。</p>

<p>决策树缺点：1、对缺失数据处理比较困难。2、容易出现过拟合问题。3、忽略数据集中属性的相互关联。4、ID3算法计算信息增益时结果偏向数值比较多的特征。
改进措施：1、对决策树进行剪枝。可以采用交叉验证法和加入正则化的方法。2、使用基于决策树的combination算法，如bagging算法，randomforest算法，可以解决过拟合的问题。</p>

<p>随机森林</p>

<p>##集成学习</p>

<p>AdaBoost算法
优点1、很好的利用了弱分类器进行级联。2、可以将不同的分类算法作为弱分类器。3、AdaBoost具有很高的精度。4、相对于bagging算法和Random Forest算法，AdaBoost充分考虑的每个分类器的权重。
缺点1、AdaBoost迭代次数也就是弱分类器数目不太好设定，可以使用交叉验证来进行确定。2、数据不平衡导致分类精度下降。3、训练比较耗时，每次重新选择当前分类器最好切分点。</p>

<p>应用领域模式识别、计算机视觉领域，用于二分类和多分类场景</p>

<p>##神经网络算法
优点1、分类准确度高，学习能力极强。2、对噪声数据鲁棒性和容错性较强。3、有联想能力，能逼近任意非线性关系。
缺点1、神经网络参数较多，权值和阈值。2、黑盒过程，不能观察中间结果。3、学习过程比较长，有可能陷入局部极小值。
应用领域目前深度神经网络已经应用与计算机视觉，自然语言处理，语音识别等领域并取得很好的效果。</p>

<p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>

<p>#生成模型（generative model）
早期：朴素贝叶斯（Naive Bayes）,高斯生成贝叶斯模型，LDA和QDA等，</p>

<p>#判别模型（discriminative model）：
logistic regression model；逻辑回归</p>

<p>分类树（classification tree）</p>

<p>支持向量机（SVM=support vector machine）</p>

<p>##trick
面对具体问题往往还需要一些降维升维和集成增强的trick，</p>

<p>有名的trick有pca,ofo,fda等降维trick，
svm专配的用来升维的kernel trick</p>

<p>树模型配套的增强trick例如bagging,random forest和boosting等。</p>

<p>##神经网络（deep learning model）</p>

<p>网络模型的结构简单明了，而且发挥空间大。</p>

<p>面对具体的问题肯定要配上具体的结构和trick，需要去看相关领域的文章总结现在popular的模型和trick</p>

<ul>
  <li>
    <p>计算机视觉（computer vision）中的分类模型好的结构基本就resnet50就能秒掉绝大多数问题，trick就有一些weight decay，bn（batch normalize），还有一些图像增强（augmentation ）的方法，dropout和early stopping以前喜欢用，但是现在工业界也没人用了</p>
  </li>
  <li>
    <p>自然语言（nlp）中的情感识别问题，则用popular的rnn结构，然后也有一些不同的trick。</p>
  </li>
</ul>

:ET